{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Machine Learning Week 3\"\nauthor: \"Nikolai Klassen\"\ndate: \"19 Oktober 2017\"\noutput: html_document\n\n---\n```{r}\nlibrary(caret)\nlibrary(ggplot2)\n```\n\n# Trees, random Forestmods, Bogging...\n\nIf data is splitble into various homogenous groups, the data is very easy to interpretate and causal interference are made easier than in complex data frames. \n\nThis data is non-linear: they suse interaction between different variables. Using data trees makes transformation less importnant. \n\nExample: Obama Clinton Divide. Homogenous groups are taken into account and their support realtive to the other homogenous groups. \n\nHow to basic algorithm:\n\n1. Start with all variables\n2. Find the variable that best separets the outcome.\n3. Divide the data into two groups: \"leaves\" on the node.\n4. Within each split, find the best variable that separets the outcome\n5. Continue until the groups are too small or sufficiently \"pure\"\n\nMeasures of impurity:\n\np is the probability for the cases \nin which either m or ca happen. \n\n\na) Misclassification Error:\n\n $p_{m,k} = \\frac{1}{N_{m}} \\sum_{in Leaf m} 1(y_i = k)$\n \n with 0 = perfect purity and 0.5 = no purity.\n \nb) Gini Index - not to be confused with the Gini Coefficient:\n \n $\\sum_{k = k?} p_{mk}*p_{mk?} = \\sum_{k = 1}^k p_{mk}(1-p_{mk}) = 1- \\sum_{k = 1}^K p^2_{mk} $\n\n1 - the squared probabilties that one case belongs to one of the other classes. \n\nc) Deviance/Information Gain:\n\n $-\\sum_{k=1}^K p_{mk}log_2 p_{mk}$\n \n- the sum of the probability of being assigend to case m in leaf k. \n\n0 = perfect purity in the leaf, 1 = no purity in the leaf\n\n### Programming a Model\n\nFunction Calls are party, rpart, and tree. \n\n### Ploting a Tree\n\n1) Making a model with train(method =\"rpart\") and ploting it with plot.\n-> model$finalmodel \n\n2) fancyRpartPlot() is fancier. \n\n\n## Bootstrap aggregating aka. bagging\n\nThe basic Idea in this approach is to resample cases and recalculate predictions in order to get an average or majority vote. It generates a similar bias with reduced variance. \n\nExample in the ozone-data.\n\n```{r \"Ozone-Example\"}\nlibrary(\"ElemStatLearn\")\ndata(\"ozone\")\nozone <- ozone[order(ozone$ozone),]\nhead(ozone)\n```\n\n### Bagged loess\n\nWere going to try to predict temperature by ozone. The Basic Idea is to create a matrix and loop over the data with sampled data from the ozone dataset. \n\n```{r Bagged Loess Code}\n\nll <- matrix(NA, nrow = 10, ncol = 155) ## loop matrix\n\nfor(i in 1:10){\n  ss <- sample(1:dim(ozone)[1], replace = T) \n  ozone0 <- ozone[ss,]; ozone0 <- ozone0[order(ozone0$ozone),]\n  loess0 <- loess(temperature~ ozone, data= ozone0, span= 0.2)\n  ll[i,] <- predict(loess0, newdata = data.frame(ozone=1:155))\n}\n\n## this makes 10 different samples and makes 10 different models averaging the effect. \n\nplot(ozone$ozone, ozone$temperature, pch =19, cex = 0.5)\n\nfor(i in 1:10){lines(1:155, ll[i,], col = \"grey\", lwd =2)} \n\nlines(1:155, apply(ll, 2, mean), col = \"red\", lwd= 2)\n\n```\n\n### More baggin in Caret\n\nwith bag()-from the caret package, \nor ctreebag()-function.\n\n## Random Forests:\n\nFrom the idea of bagging, random forests follows the same methodology. Random forests are based on bootstrap samples, but with bootstrap variables at each split. Multiple trees are being built and voted for accuracy. Contra is: speed, interpretability, and overfitting. \n-> Cross Validation needed. \n\n```{r}\ndata(iris); library(ggplot2)\ninTrain <- createDataPartition(y=iris$Species, p=0.7,list=FALSE)\ntraining <- iris[inTrain,]\ntesting <- iris[-inTrain,]\n## creating test and training data\n\nlibrary(caret)\nmodFit <- train(Species~ .,data=training,method=\"rf\",prox=TRUE)\nmodFit1\n\n```\n\n-> rfcv function makes cross val.\n\n## Boosting\n\nWiki Page https://en.wikipedia.org/wiki/AdaBoost\n\nIdea: Boosting describes the strategy behind making a weak predictor stronger by weighting and summing several weak predictors to a stronger predictor. \n\n- The Goal is to minimize error (on training set)\n- Iterative, select one h at each step\n- Calculate weights based on errors\n- Upweight missed classifications and select next h\n\nPractically:\n1. Start with a set of classfiers $h_1, ... h_k$\n2. Create a classifier that combiens the classfication functions: $f(x) = sgn(\\sum_{t =1}^T\\alpha_t h_t(x)$\n\nPros&Cons:\n\nBoosting can be used with any subset of classifiers. one large subclass is gradient boosting. Calls are:\n\nada, boost, .... \n\n### Wage Example\n\n```{r}\nlibrary(ISLR); data(Wage); library(ggplot2); library(caret);\nWage <- subset(Wage,select=-c(logwage))\ninTrain <- createDataPartition(y=Wage$wage,\n                              p=0.7, list=FALSE)\ntraining <- Wage[inTrain,]; testing <- Wage[-inTrain,]\n\nmodFit <- train(wage ~ ., method=\"gbm\",data=training,verbose=FALSE)\nprint(modFit)\n\n```\n\n```{r}\nqplot(predict(modFit, testing), wage, data = testing)\n```\n\n\n## Model based approach\n\nThe basic idea is that we assume, that the data follows a probablistic model, so that we can use Bayes? theorem to identify optimal classifiers. \n\nPros & Cons:\n\n+ Can take advantage of structure\n+ may be computionally convenient\n+ Are reasonably accurate on real problems\n\n- make additional assumptions about the data\n- when the model is incorrect you may get reduced accuracy\n\n### Technically:\n\nOur goal is to build a parametric model for conditional distribution\n$P(Y = k|X =x)$\n\n1. A typical approach is to apply Bayes theorem: According to Bayes, the possibiltie of X conditiond to Y is the following\n\n\n$Pr(Y = k| X = x) = \\frac{Pr(X = x| Y = k)Pr(Y = k)}{\\sum_{l =1}^K Pr(X =x | Y =l)Pr(Y =l)}$\n\n\nso that the distribution of the probabilties can be defined by $f_k(x)\\pi_k$. The probabilty mass function therefore is:\n\n$Pr(Y = k| X = x)=\\frac{f_k(x)\\pi_k}{\\sum_{l =1}^K f_k(x)\\pi_l}$\n\n2. The probabilties of $\\pi_k$ are set in advance from the data. \n\n3. A common choice for $f_k(x)$ is a gaussian distribution.\n\n4. Estimate the parameters for $\\mu_k, \\sigma_k^2$ from the data. \n\n5. Classifiy to the class with the highest value of $Pr(Y = k| X = x)$\n\n### Classifying using the model\n\nA range of models use this approach: linear discriminant analysis, quadratic discriminant analysis, model based predictions and naive bayes. \n\ndiscriminant functions: \nnaive bayes: \n\n#### Example in the Iris Data:\n\n```{r}\ninTrain <- createDataPartition(y=iris$Species,\n                              p=0.7, list=FALSE)\ntraining <- iris[inTrain,]\ntesting <- iris[-inTrain,]\ndim(training); dim(testing)\n\n\n```\n\nBuilding Predictions:\n\n```{r}\nmodlda = train(Species ~ .,data=training,method=\"lda\")\nmodnb = train(Species ~ ., data=training,method=\"nb\")\nplda = predict(modlda,testing); pnb = predict(modnb,testing)\ntable(plda,pnb)\n```\n\n### Quiz 3:\n\n```{r}\ninTrain <- createDataPartition(y = segmentationOriginal$Case, list = F, p = 0.6)\ntraining <- segmentationOriginal[inTrain,]\ntesting <- segmentationOriginal[-inTrain,]\nmodFit <- train(Class~., method = \"rpart\", data = training)\nmodfit$finalModel\n\n\n```\n\n\n\n\n",
    "created" : 1509630303187.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3860213808",
    "id" : "28F55F93",
    "lastKnownWriteTime" : 1509630533,
    "last_content_update" : 1509630533501,
    "path" : "F:/GitHub/Machine Learning/Script Week 3.Rmd",
    "project_path" : "Script Week 3.Rmd",
    "properties" : {
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}